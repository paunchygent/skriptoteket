---
type: reference
id: REF-hemma-incident-log-2026-01-04-153900-154030
title: "Incident log (sanitized): hemma 2026-01-04 15:39:00–15:40:30 UTC"
status: active
owners: "agents"
created: 2026-01-04
updated: 2026-01-13
topic: "devops"
---

# Incident log (sanitized): hemma 2026-01-04 15:39:00–15:40:30 UTC

This file contains a sanitized copy of the host capture:

- Source (on host): `/root/logs/incident-20260104-153900-154030.log`
- Window: **2026-01-04T15:39:00Z → 2026-01-04T15:40:30Z**

Redactions applied:

- IPv4 addresses → `<ip_redacted>`
- MAC addresses → `<mac_redacted>`
- SSIDs → `<ssid_redacted>`
- Local IPv6 addresses (`fdaa:*`, `fd7a:*`, `fe80:*`) → `<ipv6_redacted>`

Update (2026-01-13): Hemma now runs llama.cpp via Docker using `llama-server-rocm.service` (container
`llama-server-rocm`). Any `llama-server-vulkan.service` references below reflect the historical runtime during Jan 2026
debugging.

```text
incident_window_start=2026-01-04 15:39:00 UTC
incident_window_end=2026-01-04 15:40:30 UTC
capture_time_utc=2026-01-04T16:43:37Z

=== journalctl --list-boots ===
 -4 28c773ba85b94c73817a4e754e7b298d Thu 2026-01-01 10:43:18 UTC Fri 2026-01-02 08:34:55 UTC
 -3 360ebd2a0e1a4df6809db203e59501b3 Fri 2026-01-02 11:59:24 UTC Fri 2026-01-02 17:54:45 UTC
 -2 e8f86a9c0578494fa0a6260e30018f32 Sat 2026-01-03 10:39:34 UTC Sat 2026-01-03 20:55:32 UTC
 -1 63e4f220dcdd4de4b27fe54e9364af09 Sat 2026-01-03 20:58:46 UTC Sun 2026-01-04 15:40:29 UTC
  0 ee9c9259c7d84bd8858a761407e6c0c7 Sun 2026-01-04 15:57:36 UTC Sun 2026-01-04 16:43:37 UTC

=== journalctl -b -1 (system) ===
2026-01-04T15:39:06+00:00 paunchygentserver systemd[1]: Starting ssh-watchdog.service - SSH/network watchdog (logs only)...
2026-01-04T15:39:06+00:00 paunchygentserver systemd[1]: ssh-watchdog.service: Deactivated successfully.
2026-01-04T15:39:06+00:00 paunchygentserver systemd[1]: Finished ssh-watchdog.service - SSH/network watchdog (logs only).
2026-01-04T15:39:16+00:00 paunchygentserver kernel: [UFW BLOCK] IN=enp7s0 OUT= MAC=<mac_redacted>:<mac_redacted>:08:00 SRC=<ip_redacted> DST=<ip_redacted> LEN=466 TOS=0x00 PREC=0x00 TTL=64 ID=28409 DF PROTO=UDP SPT=37393 DPT=49922 LEN=446
2026-01-04T15:39:41+00:00 paunchygentserver kernel: [UFW BLOCK] IN=enp7s0 OUT= MAC=<mac_redacted>:<mac_redacted>:08:00 SRC=<ip_redacted> DST=<ip_redacted> LEN=466 TOS=0x00 PREC=0x00 TTL=64 ID=36908 DF PROTO=UDP SPT=60663 DPT=50473 LEN=446
2026-01-04T15:39:41+00:00 paunchygentserver kernel: [UFW BLOCK] IN=enp7s0 OUT= MAC=<mac_redacted>:<mac_redacted>:08:00 SRC=<ip_redacted> DST=<ip_redacted> LEN=394 TOS=0x00 PREC=0x00 TTL=64 ID=36909 DF PROTO=UDP SPT=45616 DPT=50473 LEN=374
2026-01-04T15:39:50+00:00 paunchygentserver systemd[1]: Starting heartbeat-log.service - Heartbeat log entry...
2026-01-04T15:39:51+00:00 paunchygentserver heartbeat[438925]: ok
2026-01-04T15:39:51+00:00 paunchygentserver systemd[1]: heartbeat-log.service: Deactivated successfully.
2026-01-04T15:39:51+00:00 paunchygentserver systemd[1]: Finished heartbeat-log.service - Heartbeat log entry.
2026-01-04T15:40:05+00:00 paunchygentserver kernel: [UFW BLOCK] IN=enp7s0 OUT= MAC=<mac_redacted>:<mac_redacted>:08:00 SRC=<ip_redacted> DST=<ip_redacted> LEN=466 TOS=0x00 PREC=0x00 TTL=64 ID=57939 DF PROTO=UDP SPT=55148 DPT=47417 LEN=446
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: srv  params_from_: Chat format: peg-native
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.511 (> 0.100 thold), f_keep = 0.293
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: srv  get_availabl: updating prompt cache
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: srv   prompt_save:  - saving prompt with length 2731, total state size = 426.751 MiB
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: srv          load:  - looking for better prompt, base f_keep = 0.293, sim = 0.511
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: srv        update:  - cache state: 3 prompts, 1044.767 MiB (limits: 8192.000 MiB, 16384 tokens, 52424 est)
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: srv        update:    - prompt 0x64f8e8e68ed0:    1647 tokens, checkpoints:  0,   257.364 MiB
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: srv        update:    - prompt 0x64f8e8c7ba20:    2308 tokens, checkpoints:  0,   360.652 MiB
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: srv        update:    - prompt 0x64f8e8e65cc0:    2731 tokens, checkpoints:  0,   426.751 MiB
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: srv  get_availabl: prompt cache update took 393.89 ms
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: slot launch_slot_: id  1 | task -1 | sampler chain: logits -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: slot launch_slot_: id  1 | task 915 | processing task
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: slot update_slots: id  1 | task 915 | new prompt, n_ctx_slot = 8192, n_keep = 0, task.n_tokens = 1565
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: slot update_slots: id  1 | task 915 | n_tokens = 800, memory_seq_rm [800, end)
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: slot update_slots: id  1 | task 915 | prompt processing progress, n_tokens = 1565, batch.n_tokens = 765, progress = 1.000000
2026-01-04T15:40:13+00:00 paunchygentserver llama-server[122838]: slot update_slots: id  1 | task 915 | prompt done, n_tokens = 1565, batch.n_tokens = 765
2026-01-04T15:40:18+00:00 paunchygentserver systemd[1]: Starting sysstat-collect.service - system activity accounting tool...
2026-01-04T15:40:18+00:00 paunchygentserver systemd[1]: sysstat-collect.service: Deactivated successfully.
2026-01-04T15:40:18+00:00 paunchygentserver systemd[1]: Finished sysstat-collect.service - system activity accounting tool.
2026-01-04T15:40:29+00:00 paunchygentserver llama-server[122838]: slot print_timing: id  1 | task 915 |
2026-01-04T15:40:29+00:00 paunchygentserver llama-server[122838]: prompt eval time =    1392.01 ms /   765 tokens (    1.82 ms per token,   549.56 tokens per second)
2026-01-04T15:40:29+00:00 paunchygentserver llama-server[122838]:        eval time =   14408.94 ms /   261 tokens (   55.21 ms per token,    18.11 tokens per second)
2026-01-04T15:40:29+00:00 paunchygentserver llama-server[122838]:       total time =   15800.95 ms /  1026 tokens
2026-01-04T15:40:29+00:00 paunchygentserver llama-server[122838]: slot      release: id  1 | task 915 | stop processing: n_tokens = 1825, truncated = 0
2026-01-04T15:40:29+00:00 paunchygentserver llama-server[122838]: srv  update_slots: all slots are idle
2026-01-04T15:40:29+00:00 paunchygentserver llama-server[122838]: srv  log_server_r: request: POST /v1/chat/completions <ip_redacted> 200

=== journalctl -b -1 -k (kernel) ===
2026-01-04T15:39:16+00:00 paunchygentserver kernel: [UFW BLOCK] IN=enp7s0 OUT= MAC=<mac_redacted>:<mac_redacted>:08:00 SRC=<ip_redacted> DST=<ip_redacted> LEN=466 TOS=0x00 PREC=0x00 TTL=64 ID=28409 DF PROTO=UDP SPT=37393 DPT=49922 LEN=446
2026-01-04T15:39:41+00:00 paunchygentserver kernel: [UFW BLOCK] IN=enp7s0 OUT= MAC=<mac_redacted>:<mac_redacted>:08:00 SRC=<ip_redacted> DST=<ip_redacted> LEN=466 TOS=0x00 PREC=0x00 TTL=64 ID=36908 DF PROTO=UDP SPT=60663 DPT=50473 LEN=446
2026-01-04T15:39:41+00:00 paunchygentserver kernel: [UFW BLOCK] IN=enp7s0 OUT= MAC=<mac_redacted>:<mac_redacted>:08:00 SRC=<ip_redacted> DST=<ip_redacted> LEN=394 TOS=0x00 PREC=0x00 TTL=64 ID=36909 DF PROTO=UDP SPT=45616 DPT=50473 LEN=374
2026-01-04T15:40:05+00:00 paunchygentserver kernel: [UFW BLOCK] IN=enp7s0 OUT= MAC=<mac_redacted>:<mac_redacted>:08:00 SRC=<ip_redacted> DST=<ip_redacted> LEN=466 TOS=0x00 PREC=0x00 TTL=64 ID=57939 DF PROTO=UDP SPT=55148 DPT=47417 LEN=446

=== llama-server-vulkan.service (prev boot) ===
Jan 04 15:40:13 paunchygentserver llama-server[122838]: srv  params_from_: Chat format: peg-native
Jan 04 15:40:13 paunchygentserver llama-server[122838]: slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.511 (> 0.100 thold), f_keep = 0.293
Jan 04 15:40:13 paunchygentserver llama-server[122838]: srv  get_availabl: updating prompt cache
Jan 04 15:40:13 paunchygentserver llama-server[122838]: srv   prompt_save:  - saving prompt with length 2731, total state size = 426.751 MiB
Jan 04 15:40:13 paunchygentserver llama-server[122838]: srv          load:  - looking for better prompt, base f_keep = 0.293, sim = 0.511
Jan 04 15:40:13 paunchygentserver llama-server[122838]: srv        update:  - cache state: 3 prompts, 1044.767 MiB (limits: 8192.000 MiB, 16384 tokens, 52424 est)
Jan 04 15:40:13 paunchygentserver llama-server[122838]: srv        update:    - prompt 0x64f8e8e68ed0:    1647 tokens, checkpoints:  0,   257.364 MiB
Jan 04 15:40:13 paunchygentserver llama-server[122838]: srv        update:    - prompt 0x64f8e8c7ba20:    2308 tokens, checkpoints:  0,   360.652 MiB
Jan 04 15:40:13 paunchygentserver llama-server[122838]: srv        update:    - prompt 0x64f8e8e65cc0:    2731 tokens, checkpoints:  0,   426.751 MiB
Jan 04 15:40:13 paunchygentserver llama-server[122838]: srv  get_availabl: prompt cache update took 393.89 ms
Jan 04 15:40:13 paunchygentserver llama-server[122838]: slot launch_slot_: id  1 | task -1 | sampler chain: logits -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
Jan 04 15:40:13 paunchygentserver llama-server[122838]: slot launch_slot_: id  1 | task 915 | processing task
Jan 04 15:40:13 paunchygentserver llama-server[122838]: slot update_slots: id  1 | task 915 | new prompt, n_ctx_slot = 8192, n_keep = 0, task.n_tokens = 1565
Jan 04 15:40:13 paunchygentserver llama-server[122838]: slot update_slots: id  1 | task 915 | n_tokens = 800, memory_seq_rm [800, end)
Jan 04 15:40:13 paunchygentserver llama-server[122838]: slot update_slots: id  1 | task 915 | prompt processing progress, n_tokens = 1565, batch.n_tokens = 765, progress = 1.000000
Jan 04 15:40:13 paunchygentserver llama-server[122838]: slot update_slots: id  1 | task 915 | prompt done, n_tokens = 1565, batch.n_tokens = 765
Jan 04 15:40:29 paunchygentserver llama-server[122838]: slot print_timing: id  1 | task 915 |
Jan 04 15:40:29 paunchygentserver llama-server[122838]: prompt eval time =    1392.01 ms /   765 tokens (    1.82 ms per token,   549.56 tokens per second)
Jan 04 15:40:29 paunchygentserver llama-server[122838]:        eval time =   14408.94 ms /   261 tokens (   55.21 ms per token,    18.11 tokens per second)
Jan 04 15:40:29 paunchygentserver llama-server[122838]:       total time =   15800.95 ms /  1026 tokens
Jan 04 15:40:29 paunchygentserver llama-server[122838]: slot      release: id  1 | task 915 | stop processing: n_tokens = 1825, truncated = 0
Jan 04 15:40:29 paunchygentserver llama-server[122838]: srv  update_slots: all slots are idle
Jan 04 15:40:29 paunchygentserver llama-server[122838]: srv  log_server_r: request: POST /v1/chat/completions <ip_redacted> 200

=== rocm-smi (current boot) ===


============================ ROCm System Management Interface ============================
=================================== % time GPU is busy ===================================
GPU[0]		: GPU use (%): 0
==========================================================================================
=================================== Current Memory Use ===================================
GPU[0]		: GPU Memory Allocated (VRAM%): 85
GPU[0]		: GPU Memory Read/Write Activity (%): 0
GPU[0]		: Memory Activity: N/A
GPU[0]		: Avg. Memory Bandwidth: 0
==========================================================================================
===================================== KFD Processes ======================================
No KFD PIDs currently running
==========================================================================================
================================== End of ROCm SMI Log ===================================
```
